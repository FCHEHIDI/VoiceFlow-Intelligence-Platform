version: '3.8'

services:
  # Rust Inference Server
  voiceflow-inference:
    build:
      context: ./voiceflow-inference
      dockerfile: Dockerfile
    container_name: voiceflow-inference-server
    environment:
      - RUST_LOG=info
      - MODEL_PATH=/models/fast_cnn_diarization_optimized.onnx
      - PORT=3000
      - WORKERS=4
    volumes:
      - ./voiceflow-ml/models:/models:ro  # Mount models read-only
      - ./logs:/logs                       # Mount logs directory
    ports:
      - "3000:3000"  # HTTP/WebSocket API
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - voiceflow-network

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: voiceflow-prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    networks:
      - voiceflow-network
    restart: unless-stopped

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: voiceflow-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    networks:
      - voiceflow-network
    restart: unless-stopped

volumes:
  prometheus-data:
  grafana-data:

networks:
  voiceflow-network:
    driver: bridge
