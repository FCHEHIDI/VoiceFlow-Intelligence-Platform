# GPU-enabled Dockerfile for ONNX Runtime with CUDA support
# Optimized for GCP T4, A100, or local RTX GPUs

FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Metadata
LABEL maintainer="VoiceFlow Team"
LABEL description="GPU-accelerated speaker diarization inference"
LABEL version="1.0.0"

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_VISIBLE_DEVICES=0 \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Install Python 3.10 and dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    python3.10-dev \
    curl \
    wget \
    git \
    libsndfile1 \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Create Python symlink
RUN ln -sf /usr/bin/python3.10 /usr/bin/python

# Upgrade pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Install ONNX Runtime GPU (CUDA 11.8 compatible)
# Version 1.16+ supports CUDA 11.8 and TensorRT 8.6
RUN pip install --no-cache-dir \
    onnxruntime-gpu==1.16.3 \
    numpy==1.24.3 \
    scipy==1.11.3 \
    soundfile==0.12.1 \
    fastapi==0.104.1 \
    uvicorn[standard]==0.24.0 \
    pydantic==2.5.0 \
    prometheus-client==0.19.0

# Create application directory
WORKDIR /app

# Copy model files
COPY models/ /models/

# Copy inference server code (if you have Python wrapper)
COPY scripts/inference_server.py /app/

# Create non-root user for security
RUN useradd -m -u 1000 voiceflow && \
    chown -R voiceflow:voiceflow /app /models

USER voiceflow

# Expose ports
EXPOSE 8080 9090

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:9090/health || exit 1

# Run inference server
CMD ["python", "inference_server.py"]
