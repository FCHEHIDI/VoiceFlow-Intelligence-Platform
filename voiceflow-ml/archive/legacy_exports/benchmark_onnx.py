"""
Benchmark ONNX model inference speed.

Tests both unoptimized and optimized models to measure:
- Cold start latency
- Warm inference latency (P50, P95, P99)
- Throughput
"""

import onnxruntime as ort
import numpy as np
import time
from pathlib import Path
from statistics import mean, median


def benchmark_model(model_path: Path, num_warmup: int = 10, num_iterations: int = 100):
    """Benchmark ONNX model inference speed."""
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š Benchmarking: {model_path.name}")
    print(f"{'=' * 60}")
    
    # 1. Load model
    print(f"\n1ï¸âƒ£ Loading model...")
    start = time.perf_counter()
    
    session = ort.InferenceSession(
        str(model_path),
        providers=['CPUExecutionProvider']
    )
    
    load_time = (time.perf_counter() - start) * 1000
    print(f"   â””â”€ Load time: {load_time:.1f} ms")
    
    # 2. Create test input
    dummy_input = np.random.randn(1, 48000).astype(np.float32)  # 3 seconds audio
    
    # 3. Warmup runs
    print(f"\n2ï¸âƒ£ Warming up ({num_warmup} runs)...")
    for i in range(num_warmup):
        session.run(None, {'audio': dummy_input})
    print(f"   âœ“ Warmup complete")
    
    # 4. Benchmark runs
    print(f"\n3ï¸âƒ£ Benchmarking ({num_iterations} runs)...")
    latencies = []
    
    for i in range(num_iterations):
        start = time.perf_counter()
        outputs = session.run(None, {'audio': dummy_input})
        end = time.perf_counter()
        
        latencies.append((end - start) * 1000)  # Convert to ms
    
    # 5. Calculate statistics
    latencies.sort()
    
    p50 = latencies[len(latencies) // 2]
    p95 = latencies[int(len(latencies) * 0.95)]
    p99 = latencies[int(len(latencies) * 0.99)]
    avg = mean(latencies)
    min_lat = min(latencies)
    max_lat = max(latencies)
    
    print(f"\n4ï¸âƒ£ Results:")
    print(f"   â”œâ”€ Min:     {min_lat:.2f} ms")
    print(f"   â”œâ”€ Average: {avg:.2f} ms")
    print(f"   â”œâ”€ Median:  {p50:.2f} ms")
    print(f"   â”œâ”€ P95:     {p95:.2f} ms")
    print(f"   â”œâ”€ P99:     {p99:.2f} ms")
    print(f"   â””â”€ Max:     {max_lat:.2f} ms")
    
    # 6. Performance assessment
    print(f"\n5ï¸âƒ£ Performance Assessment:")
    
    if p99 < 100:
        print(f"   âœ… EXCELLENT: P99 latency < 100ms target!")
        print(f"      â””â”€ Production ready for real-time inference")
    elif p99 < 200:
        print(f"   âš   GOOD: P99 latency < 200ms (acceptable)")
        print(f"      â””â”€ Consider quantization for further speedup")
    elif p99 < 500:
        print(f"   âš   MODERATE: P99 latency < 500ms")
        print(f"      â””â”€ Needs optimization (quantization, GPU, model pruning)")
    else:
        print(f"   âŒ SLOW: P99 latency > 500ms")
        print(f"      â””â”€ Requires significant optimization")
    
    # 7. Throughput
    throughput = 1000 / avg  # Requests per second
    print(f"\n6ï¸âƒ£ Throughput:")
    print(f"   â””â”€ {throughput:.1f} requests/sec (single thread)")
    
    return {
        'load_time': load_time,
        'min': min_lat,
        'avg': avg,
        'median': p50,
        'p95': p95,
        'p99': p99,
        'max': max_lat,
        'throughput': throughput
    }


def compare_models():
    """Compare unoptimized vs optimized models."""
    
    unopt_path = Path("../models/diarization_transformer.onnx")
    opt_path = Path("../models/diarization_transformer_optimized.onnx")
    
    if not unopt_path.exists():
        print(f"âŒ Unoptimized model not found: {unopt_path}")
        return
    
    if not opt_path.exists():
        print(f"âŒ Optimized model not found: {opt_path}")
        return
    
    print("=" * 60)
    print("ğŸš€ ONNX Model Performance Benchmark")
    print("=" * 60)
    print(f"\nTest configuration:")
    print(f"â”œâ”€ Input: 3 seconds audio (48000 samples @ 16kHz)")
    print(f"â”œâ”€ Hardware: CPU")
    print(f"â”œâ”€ Provider: CPUExecutionProvider")
    print(f"â””â”€ Iterations: 100")
    
    # Benchmark unoptimized
    unopt_results = benchmark_model(unopt_path)
    
    # Benchmark optimized
    opt_results = benchmark_model(opt_path)
    
    # Compare
    print(f"\n{'=' * 60}")
    print(f"ğŸ“ˆ Comparison Summary")
    print(f"{'=' * 60}")
    
    print(f"\nLoad Time:")
    print(f"â”œâ”€ Unoptimized: {unopt_results['load_time']:.1f} ms")
    print(f"â””â”€ Optimized:   {opt_results['load_time']:.1f} ms")
    
    print(f"\nP99 Latency (target: <100ms):")
    print(f"â”œâ”€ Unoptimized: {unopt_results['p99']:.2f} ms")
    print(f"â””â”€ Optimized:   {opt_results['p99']:.2f} ms")
    
    speedup = unopt_results['p99'] / opt_results['p99']
    print(f"\nSpeedup: {speedup:.2f}x faster")
    
    print(f"\nThroughput:")
    print(f"â”œâ”€ Unoptimized: {unopt_results['throughput']:.1f} req/sec")
    print(f"â””â”€ Optimized:   {opt_results['throughput']:.1f} req/sec")
    
    # Recommendation
    print(f"\n{'=' * 60}")
    print(f"ğŸ’¡ Recommendation")
    print(f"{'=' * 60}")
    
    if opt_results['p99'] < 100:
        print(f"\nâœ… Use optimized model for production!")
        print(f"   â”œâ”€ Meets <100ms P99 latency requirement")
        print(f"   â”œâ”€ Ready for real-time inference")
        print(f"   â””â”€ File: {opt_path.name}")
    elif unopt_results['p99'] < 100 and opt_results['p99'] < 200:
        print(f"\nâœ… Both models perform well!")
        print(f"   â”œâ”€ Optimized model: {opt_path.name}")
        print(f"   â””â”€ Trade-off: {opt_results['load_time']:.0f}ms load time vs {opt_results['p99']:.0f}ms inference")
    else:
        print(f"\nâš   Consider further optimizations:")
        print(f"   â”œâ”€ INT8 quantization (2-4x faster)")
        print(f"   â”œâ”€ FP16 on GPU (5-10x faster)")
        print(f"   â”œâ”€ Model distillation (smaller Wav2Vec2)")
        print(f"   â””â”€ TensorRT/OpenVINO compilation")


if __name__ == "__main__":
    compare_models()
