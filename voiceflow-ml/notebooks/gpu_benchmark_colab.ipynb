{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef80bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ede678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install onnxruntime-gpu numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560526a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your model file: diarization_transformer_optimized.onnx\n",
    "# Or download from your repo:\n",
    "# !wget https://github.com/FCHEHIDI/VoiceFlow-Intelligence-Platform/raw/main/VoiceFlow-Intelligence-Platform/voiceflow-ml/models/diarization_transformer_optimized.onnx\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload your .onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef87965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "MODEL_PATH = \"diarization_transformer_optimized.onnx\"  # Adjust if needed\n",
    "\n",
    "def benchmark_inference(provider: str, iterations: int = 200) -> dict:\n",
    "    \"\"\"\n",
    "    Benchmark ONNX inference with specified provider.\n",
    "    \n",
    "    Args:\n",
    "        provider: 'CUDAExecutionProvider' or 'CPUExecutionProvider'\n",
    "        iterations: Number of inference runs\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Benchmarking with {provider}...\")\n",
    "    \n",
    "    # Create session\n",
    "    session = ort.InferenceSession(\n",
    "        MODEL_PATH,\n",
    "        providers=[provider]\n",
    "    )\n",
    "    \n",
    "    # Get input shape\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape\n",
    "    \n",
    "    # Generate random input (adjust shape if needed)\n",
    "    # Assuming input is (batch_size, sequence_length, features)\n",
    "    if input_shape[0] is None or isinstance(input_shape[0], str):\n",
    "        input_shape[0] = 1  # Batch size\n",
    "    \n",
    "    test_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"Warming up...\")\n",
    "    for _ in range(10):\n",
    "        session.run(None, {input_name: test_input})\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"Running {iterations} iterations...\")\n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        session.run(None, {input_name: test_input})\n",
    "        latency_ms = (time.perf_counter() - start) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{iterations}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    latencies_sorted = sorted(latencies)\n",
    "    results = {\n",
    "        \"provider\": provider,\n",
    "        \"iterations\": iterations,\n",
    "        \"min_ms\": min(latencies),\n",
    "        \"max_ms\": max(latencies),\n",
    "        \"mean_ms\": np.mean(latencies),\n",
    "        \"median_ms\": np.median(latencies),\n",
    "        \"p50_ms\": latencies_sorted[len(latencies) // 2],\n",
    "        \"p95_ms\": latencies_sorted[int(len(latencies) * 0.95)],\n",
    "        \"p99_ms\": latencies_sorted[int(len(latencies) * 0.99)],\n",
    "        \"throughput_rps\": 1000 / np.mean(latencies)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Pretty print benchmark results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Provider: {results['provider']}\")\n",
    "    print(f\"Iterations: {results['iterations']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Min:        {results['min_ms']:.2f} ms\")\n",
    "    print(f\"Mean:       {results['mean_ms']:.2f} ms\")\n",
    "    print(f\"Median:     {results['median_ms']:.2f} ms\")\n",
    "    print(f\"P50:        {results['p50_ms']:.2f} ms\")\n",
    "    print(f\"P95:        {results['p95_ms']:.2f} ms\")\n",
    "    print(f\"P99:        {results['p99_ms']:.2f} ms ‚≠ê\")\n",
    "    print(f\"Max:        {results['max_ms']:.2f} ms\")\n",
    "    print(f\"Throughput: {results['throughput_rps']:.1f} req/s\")\n",
    "    print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GPU benchmark\n",
    "gpu_results = benchmark_inference('CUDAExecutionProvider', iterations=200)\n",
    "print_results(gpu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CPU benchmark for comparison\n",
    "cpu_results = benchmark_inference('CPUExecutionProvider', iterations=200)\n",
    "print_results(cpu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "print(\"\\nüéØ GPU vs CPU Comparison\")\n",
    "print(f\"{'='*60}\")\n",
    "speedup = cpu_results['mean_ms'] / gpu_results['mean_ms']\n",
    "print(f\"GPU P99:     {gpu_results['p99_ms']:.2f} ms\")\n",
    "print(f\"CPU P99:     {cpu_results['p99_ms']:.2f} ms\")\n",
    "print(f\"Speedup:     {speedup:.1f}x faster on GPU\")\n",
    "print(f\"GPU Target:  < 10 ms P99 {'‚úÖ' if gpu_results['p99_ms'] < 10 else '‚ùå'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open('benchmark_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'gpu': gpu_results,\n",
    "        'cpu': cpu_results,\n",
    "        'speedup': speedup\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Results saved to benchmark_results.json\")\n",
    "\n",
    "# Download results\n",
    "files.download('benchmark_results.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
