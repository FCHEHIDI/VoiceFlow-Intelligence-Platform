{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f23a70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 1 root root 4.0K Nov 20 14:30 .\n",
      "drwxr-xr-x 1 root root 4.0K Dec  8 15:39 ..\n",
      "drwxr-xr-x 4 root root 4.0K Nov 20 14:30 .config\n",
      "drwxr-xr-x 1 root root 4.0K Nov 20 14:30 sample_data\n"
     ]
    }
   ],
   "source": [
    "# List all files in Colab\n",
    "!ls -lha /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40b2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /content\n",
      "Files: ['.config', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "# Check current directory\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Files: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef80bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  8 15:47:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ede678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install onnxruntime-gpu numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "560526a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Model not found at: diarization_transformer_optimized.onnx\n",
      "Current directory: /content\n",
      "All files: ['.config', 'sample_data']\n",
      "\n",
      "üí° Please use Colab's file browser (folder icon on left) to upload:\n",
      "   voiceflow-ml/notebooks/diarization_transformer_optimized.onnx\n"
     ]
    }
   ],
   "source": [
    "# Set model path\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"diarization_transformer_optimized.onnx\"\n",
    "\n",
    "# Check if exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    size_kb = os.path.getsize(MODEL_PATH) / 1024\n",
    "    print(f\"‚úÖ Model found: {size_kb:.1f} KB\")\n",
    "else:\n",
    "    # List all files to debug\n",
    "    print(f\"‚ùå Model not found at: {MODEL_PATH}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"All files: {os.listdir('.')}\")\n",
    "    \n",
    "    # Try to find any .onnx files\n",
    "    import glob\n",
    "    onnx_files = glob.glob(\"**/*.onnx\", recursive=True)\n",
    "    if onnx_files:\n",
    "        print(f\"Found .onnx files: {onnx_files}\")\n",
    "        MODEL_PATH = onnx_files[0]\n",
    "        print(f\"Using: {MODEL_PATH}\")\n",
    "    else:\n",
    "        print(\"\\nüí° Please use Colab's file browser (folder icon on left) to upload:\")\n",
    "        print(\"   voiceflow-ml/notebooks/diarization_transformer_optimized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef87965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "def benchmark_inference(model_path: str, provider: str, iterations: int = 200) -> dict:\n",
    "    \"\"\"\n",
    "    Benchmark ONNX inference with specified provider.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to ONNX model file\n",
    "        provider: 'CUDAExecutionProvider' or 'CPUExecutionProvider'\n",
    "        iterations: Number of inference runs\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Benchmarking with {provider}...\")\n",
    "    \n",
    "    # Create session\n",
    "    session = ort.InferenceSession(\n",
    "        model_path,\n",
    "        providers=[provider]\n",
    "    )\n",
    "    \n",
    "    # Get input shape\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = list(session.get_inputs()[0].shape)\n",
    "    \n",
    "    print(f\"Model input: {input_name}, shape: {input_shape}\")\n",
    "    \n",
    "    # Generate random input (adjust shape if needed)\n",
    "    # Assuming input is (batch_size, sequence_length, features)\n",
    "    if input_shape[0] is None or isinstance(input_shape[0], str):\n",
    "        input_shape[0] = 1  # Batch size\n",
    "    \n",
    "    test_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"Warming up...\")\n",
    "    for _ in range(10):\n",
    "        session.run(None, {input_name: test_input})\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"Running {iterations} iterations...\")\n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        session.run(None, {input_name: test_input})\n",
    "        latency_ms = (time.perf_counter() - start) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{iterations}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    latencies_sorted = sorted(latencies)\n",
    "    results = {\n",
    "        \"provider\": provider,\n",
    "        \"iterations\": iterations,\n",
    "        \"min_ms\": min(latencies),\n",
    "        \"max_ms\": max(latencies),\n",
    "        \"mean_ms\": np.mean(latencies),\n",
    "        \"median_ms\": np.median(latencies),\n",
    "        \"p50_ms\": latencies_sorted[len(latencies) // 2],\n",
    "        \"p95_ms\": latencies_sorted[int(len(latencies) * 0.95)],\n",
    "        \"p99_ms\": latencies_sorted[int(len(latencies) * 0.99)],\n",
    "        \"throughput_rps\": 1000 / np.mean(latencies)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results(results: dict):\n",
    "    \"\"\"Pretty print benchmark results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Provider: {results['provider']}\")\n",
    "    print(f\"Iterations: {results['iterations']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Min:        {results['min_ms']:.2f} ms\")\n",
    "    print(f\"Mean:       {results['mean_ms']:.2f} ms\")\n",
    "    print(f\"Median:     {results['median_ms']:.2f} ms\")\n",
    "    print(f\"P50:        {results['p50_ms']:.2f} ms\")\n",
    "    print(f\"P95:        {results['p95_ms']:.2f} ms\")\n",
    "    print(f\"P99:        {results['p99_ms']:.2f} ms ‚≠ê\")\n",
    "    print(f\"Max:        {results['max_ms']:.2f} ms\")\n",
    "    print(f\"Throughput: {results['throughput_rps']:.1f} req/s\")\n",
    "    print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1283fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Benchmarking with CUDAExecutionProvider...\n"
     ]
    },
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from diarization_transformer_optimized.onnx failed:Load model diarization_transformer_optimized.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1758625316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run GPU benchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpu_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CUDAExecutionProvider'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4063836931.py\u001b[0m in \u001b[0;36mbenchmark_inference\u001b[0;34m(model_path, provider, iterations)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Create session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     session = ort.InferenceSession(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mproviders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_inference_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisabled_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from diarization_transformer_optimized.onnx failed:Load model diarization_transformer_optimized.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "# Run GPU benchmark\n",
    "gpu_results = benchmark_inference(MODEL_PATH, 'CUDAExecutionProvider', iterations=200)\n",
    "print_results(gpu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CPU benchmark for comparison\n",
    "cpu_results = benchmark_inference(MODEL_PATH, 'CPUExecutionProvider', iterations=200)\n",
    "print_results(cpu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "print(\"\\nüéØ GPU vs CPU Comparison\")\n",
    "print(f\"{'='*60}\")\n",
    "speedup = cpu_results['mean_ms'] / gpu_results['mean_ms']\n",
    "print(f\"GPU P99:     {gpu_results['p99_ms']:.2f} ms\")\n",
    "print(f\"CPU P99:     {cpu_results['p99_ms']:.2f} ms\")\n",
    "print(f\"Speedup:     {speedup:.1f}x faster on GPU\")\n",
    "print(f\"GPU Target:  < 10 ms P99 {'‚úÖ' if gpu_results['p99_ms'] < 10 else '‚ùå'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "results_file = 'benchmark_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'gpu': gpu_results,\n",
    "        'cpu': cpu_results,\n",
    "        'speedup': speedup\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to {results_file}\")\n",
    "\n",
    "# Download results (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(results_file)\n",
    "    print(\"üì• Download started!\")\n",
    "except ImportError:\n",
    "    print(f\"üí° Running locally - results saved to: {os.path.abspath(results_file)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
