{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory for MFCC experiments\n",
    "checkpoint_dir = '/content/drive/MyDrive/voiceflow_mfcc_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(f\"âœ… MFCC Checkpoints will be saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394b2a4",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (includes librosa for MFCC extraction)\n",
    "!pip install -q datasets[audio] torch torchaudio transformers accelerate librosa soundfile\n",
    "\n",
    "# Note: librosa provides MFCC extraction and delta computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca9886",
   "metadata": {},
   "source": [
    "## Step 3: Clone Model Code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8400f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if already cloned\n",
    "repo_root = '/content/VoiceFlow-Intelligence-Platform'\n",
    "if os.path.exists(repo_root):\n",
    "    print(\"âœ… Repository already exists, pulling latest changes...\")\n",
    "    !cd /content/VoiceFlow-Intelligence-Platform && git pull origin main\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    %cd /content\n",
    "    !git clone https://github.com/FCHEHIDI/VoiceFlow-Intelligence-Platform.git\n",
    "    \n",
    "# Verify the structure and find the correct path\n",
    "print(\"\\nðŸ” Detecting repository structure...\")\n",
    "\n",
    "# Colab sometimes creates deeply nested structures. Check all possibilities:\n",
    "possible_paths = [\n",
    "    '/content/VoiceFlow-Intelligence-Platform/voiceflow-ml/VoiceFlow-Intelligence-Platform/voiceflow-ml',  # Double-nested\n",
    "    '/content/VoiceFlow-Intelligence-Platform/VoiceFlow-Intelligence-Platform/voiceflow-ml',  # Nested\n",
    "    '/content/VoiceFlow-Intelligence-Platform/voiceflow-ml',  # Flat\n",
    "]\n",
    "\n",
    "voiceflow_ml_path = None\n",
    "for path in possible_paths:\n",
    "    models_path = os.path.join(path, 'models')\n",
    "    if os.path.exists(path) and os.path.exists(models_path):\n",
    "        voiceflow_ml_path = path\n",
    "        print(f\"âœ… Found voiceflow-ml with models/ at: {path}\")\n",
    "        break\n",
    "\n",
    "if not voiceflow_ml_path:\n",
    "    # Debugging: show actual structure at each level\n",
    "    print(f\"\\nâŒ Could not find voiceflow-ml with models/ directory\")\n",
    "    print(f\"\\nðŸ“‚ /content/VoiceFlow-Intelligence-Platform/:\")\n",
    "    !ls -la /content/VoiceFlow-Intelligence-Platform/\n",
    "    \n",
    "    # Check voiceflow-ml directory\n",
    "    if os.path.exists('/content/VoiceFlow-Intelligence-Platform/voiceflow-ml'):\n",
    "        print(f\"\\nðŸ“‚ /content/VoiceFlow-Intelligence-Platform/voiceflow-ml/:\")\n",
    "        !ls -la /content/VoiceFlow-Intelligence-Platform/voiceflow-ml/\n",
    "        \n",
    "        # Check for nested repo inside voiceflow-ml\n",
    "        nested_repo = '/content/VoiceFlow-Intelligence-Platform/voiceflow-ml/VoiceFlow-Intelligence-Platform'\n",
    "        if os.path.exists(nested_repo):\n",
    "            print(f\"\\nðŸ“‚ Found nested repo at: {nested_repo}\")\n",
    "            print(f\"Contents:\")\n",
    "            !ls -la {nested_repo}\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find voiceflow-ml/models/ - see directory listings above\")\n",
    "\n",
    "# Verify critical files exist\n",
    "models_path = os.path.join(voiceflow_ml_path, 'models')\n",
    "diarization_path = os.path.join(models_path, 'diarization')\n",
    "model_file = os.path.join(diarization_path, 'model.py')\n",
    "\n",
    "print(f\"âœ… models/ directory: {models_path}\")\n",
    "print(f\"âœ… models/diarization/ directory: {diarization_path}\")\n",
    "print(f\"âœ… models/diarization/model.py: {'exists' if os.path.exists(model_file) else 'MISSING'}\")\n",
    "\n",
    "if not os.path.exists(model_file):\n",
    "    print(f\"\\nâŒ ERROR: model.py not found!\")\n",
    "    print(f\"\\nContents of models/diarization/:\")\n",
    "    !ls -la {diarization_path}\n",
    "    raise FileNotFoundError(f\"model.py not found at {model_file}\")\n",
    "\n",
    "# Store the path for next cells\n",
    "%store voiceflow_ml_path\n",
    "print(f\"\\nâœ… Repository verified successfully!\")\n",
    "print(f\"ðŸ“ Using: {voiceflow_ml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f37772",
   "metadata": {},
   "source": [
    "## Step 4: Load Streaming Dataset (Zero Local Storage!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70822c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Dataset selection\n",
    "DATASET_NAME = \"librispeech_asr\"\n",
    "print(f\"Loading {DATASET_NAME} in STREAMING mode...\")\n",
    "print(\"âš ï¸ No download required - audio streams on-demand!\\n\")\n",
    "\n",
    "# Load train split (streaming)\n",
    "train_dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    \"clean\",  # LibriSpeech subset\n",
    "    split=\"train.360\",  # 360 hours\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load validation split\n",
    "val_dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    \"clean\",\n",
    "    split=\"validation\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Cast audio column to ensure proper decoding\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000, decode=True))\n",
    "val_dataset = val_dataset.cast_column(\"audio\", Audio(sampling_rate=16000, decode=True))\n",
    "\n",
    "print(\"âœ… Datasets ready (streaming mode)\")\n",
    "print(f\"Local storage used: 0 GB âœ…\\n\")\n",
    "\n",
    "# Inspect first sample\n",
    "try:\n",
    "    sample = next(iter(train_dataset))\n",
    "    print(\"Sample keys:\", sample.keys())\n",
    "    print(f\"Audio shape: {sample['audio']['array'].shape}\")\n",
    "    print(f\"Sample rate: {sample['audio']['sampling_rate']} Hz\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Sample inspection failed: {e}\")\n",
    "    print(\"This is OK - audio will decode when training starts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac972d",
   "metadata": {},
   "source": [
    "## Step 5: Load Enhanced Model with MFCC Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Retrieve the voiceflow_ml_path from Step 3\n",
    "try:\n",
    "    %store -r voiceflow_ml_path\n",
    "    print(f\"âœ… Using voiceflow-ml path from Step 3: {voiceflow_ml_path}\")\n",
    "except:\n",
    "    # Fallback: try to find it again\n",
    "    print(\"âš ï¸ Path not stored, searching...\")\n",
    "    possible_paths = [\n",
    "        '/content/VoiceFlow-Intelligence-Platform/voiceflow-ml',\n",
    "        '/content/VoiceFlow-Intelligence-Platform/VoiceFlow-Intelligence-Platform/voiceflow-ml',\n",
    "    ]\n",
    "    voiceflow_ml_path = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(os.path.join(path, 'models')):\n",
    "            voiceflow_ml_path = path\n",
    "            break\n",
    "    \n",
    "    if not voiceflow_ml_path:\n",
    "        raise FileNotFoundError(\"Could not find voiceflow-ml directory. Please re-run Step 3.\")\n",
    "\n",
    "# Add to Python path\n",
    "if voiceflow_ml_path not in sys.path:\n",
    "    sys.path.insert(0, voiceflow_ml_path)\n",
    "\n",
    "print(f\"âœ… Python path configured: {voiceflow_ml_path}\")\n",
    "\n",
    "# Verify models directory exists\n",
    "models_path = os.path.join(voiceflow_ml_path, 'models')\n",
    "if not os.path.exists(models_path):\n",
    "    print(f\"\\nâŒ ERROR: models/ directory not found!\")\n",
    "    print(f\"Expected location: {models_path}\")\n",
    "    print(f\"\\nContents of {voiceflow_ml_path}:\")\n",
    "    !ls -la {voiceflow_ml_path}\n",
    "    raise FileNotFoundError(f\"models/ directory not found at {models_path}\")\n",
    "\n",
    "print(f\"âœ… models/ directory found: {models_path}\")\n",
    "\n",
    "# Now import the model\n",
    "try:\n",
    "    from models.diarization.model import FastDiarizationModel\n",
    "    print(\"âœ… Model imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ Import failed: {e}\")\n",
    "    print(f\"\\nDebugging info:\")\n",
    "    print(f\"  Current directory: {os.getcwd()}\")\n",
    "    print(f\"  Python sys.path[0]: {sys.path[0]}\")\n",
    "    print(f\"  models/ exists: {os.path.exists(models_path)}\")\n",
    "    print(f\"  models/__init__.py exists: {os.path.exists(os.path.join(models_path, '__init__.py'))}\")\n",
    "    print(f\"  models/diarization/ exists: {os.path.exists(os.path.join(models_path, 'diarization'))}\")\n",
    "    print(f\"  models/diarization/model.py exists: {os.path.exists(os.path.join(models_path, 'diarization/model.py'))}\")\n",
    "    print(f\"\\nContents of models/:\")\n",
    "    !ls -la {models_path}\n",
    "    raise\n",
    "\n",
    "# Model configuration with MFCC input channels\n",
    "CONFIG = {\n",
    "    'num_speakers': 2,\n",
    "    'hidden_size': 256,\n",
    "    'encoder_type': 'lightweight-cnn',\n",
    "    'dropout': 0.3,\n",
    "    'in_channels': 80,  # ðŸ”‘ NEW: 40 MFCCs + 40 deltas\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FastDiarizationModel(**CONFIG).to(device)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model: FastDiarizationModel (Enhanced CNN with MFCC)\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Input: 80 channels (40 MFCCs + 40 deltas)\")\n",
    "print(f\"Parameters: {model.count_parameters() / 1e6:.2f}M\")\n",
    "print(f\"Trainable: {model.count_trainable_parameters() / 1e6:.2f}M\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d096c2",
   "metadata": {},
   "source": [
    "## Step 6: Create MFCC Streaming DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class StreamingAudioDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Wrapper for streaming HuggingFace dataset with MFCC feature extraction.\n",
    "    \n",
    "    Extracts 40 MFCC coefficients + 40 delta coefficients = 80 features.\n",
    "    This replaces raw waveform input with acoustic features for better accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, target_sr=16000, duration=3.0, max_samples=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.target_sr = target_sr\n",
    "        self.target_length = int(target_sr * duration)\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        # MFCC parameters (industry standard)\n",
    "        self.n_mfcc = 40  # 40 coefficients\n",
    "        self.n_fft = 512  # FFT window\n",
    "        self.hop_length = 160  # 10ms hop @ 16kHz\n",
    "        self.n_mels = 80  # Mel filterbank size\n",
    "        \n",
    "        # Pre-scan dataset to create persistent label mapping\n",
    "        print(\"ðŸ” Pre-scanning dataset to create stable label mapping...\")\n",
    "        self.speaker_to_label = {}\n",
    "        self._create_label_mapping()\n",
    "        print(f\"âœ… Mapped {len(self.speaker_to_label)} speakers to 2 labels\")\n",
    "        print(f\"   Label 0: {sum(1 for l in self.speaker_to_label.values() if l == 0)} speakers\")\n",
    "        print(f\"   Label 1: {sum(1 for l in self.speaker_to_label.values() if l == 1)} speakers\")\n",
    "    \n",
    "    def _create_label_mapping(self):\n",
    "        \"\"\"Pre-scan dataset once to build stable speaker-to-label mapping.\"\"\"\n",
    "        next_label = 0\n",
    "        count = 0\n",
    "        \n",
    "        for sample in self.dataset:\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "            \n",
    "            speaker_id = str(sample.get('speaker_id', 0))\n",
    "            if speaker_id not in self.speaker_to_label:\n",
    "                self.speaker_to_label[speaker_id] = next_label\n",
    "                next_label = (next_label + 1) % CONFIG['num_speakers']\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    def _extract_mfcc_features(self, audio_np):\n",
    "        \"\"\"\n",
    "        Extract MFCC features from audio waveform.\n",
    "        \n",
    "        Args:\n",
    "            audio_np: numpy array of audio samples (1D)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor of shape (80, n_frames):\n",
    "                - 40 MFCC coefficients\n",
    "                - 40 delta coefficients\n",
    "        \"\"\"\n",
    "        # Extract 40 MFCCs\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=audio_np,\n",
    "            sr=self.target_sr,\n",
    "            n_mfcc=self.n_mfcc,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            n_mels=self.n_mels\n",
    "        )  # Shape: (40, n_frames)\n",
    "        \n",
    "        # Compute delta features (temporal dynamics)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)  # Shape: (40, n_frames)\n",
    "        \n",
    "        # Stack to create 80-channel input\n",
    "        features = np.vstack([mfcc, mfcc_delta])  # Shape: (80, n_frames)\n",
    "        \n",
    "        return torch.FloatTensor(features)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        for sample in self.dataset:\n",
    "            # Stop if max_samples reached\n",
    "            if self.max_samples and count >= self.max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Extract audio\n",
    "                audio = torch.FloatTensor(sample['audio']['array'])\n",
    "                sr = sample['audio']['sampling_rate']\n",
    "                \n",
    "                # Resample to 16kHz if needed\n",
    "                if sr != self.target_sr:\n",
    "                    resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "                    audio = resampler(audio)\n",
    "                \n",
    "                # Pad/crop to fixed length\n",
    "                if audio.shape[0] > self.target_length:\n",
    "                    # Random crop\n",
    "                    start = np.random.randint(0, audio.shape[0] - self.target_length)\n",
    "                    audio = audio[start:start + self.target_length]\n",
    "                elif audio.shape[0] < self.target_length:\n",
    "                    # Zero-pad\n",
    "                    audio = torch.nn.functional.pad(audio, (0, self.target_length - audio.shape[0]))\n",
    "                \n",
    "                # ðŸ”‘ NEW: Extract MFCC features instead of raw waveform\n",
    "                audio_np = audio.numpy()\n",
    "                features = self._extract_mfcc_features(audio_np)\n",
    "                # features shape: (80, ~300 frames) for 3-second audio\n",
    "                \n",
    "                # Use persistent label mapping\n",
    "                speaker_id = str(sample.get('speaker_id', 0))\n",
    "                label = self.speaker_to_label.get(speaker_id, 0)\n",
    "                \n",
    "                yield features, label\n",
    "                count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Skip corrupted samples\n",
    "                print(f\"Skipping sample due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "# Create streaming datasets with MFCC extraction\n",
    "print(\"ðŸŽµ Creating MFCC streaming datasets...\")\n",
    "train_streaming = StreamingAudioDataset(train_dataset, max_samples=10000)\n",
    "val_streaming = StreamingAudioDataset(val_dataset, max_samples=1000)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for 2D MFCC features.\"\"\"\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    # Pad features to same length (handle variable frame counts)\n",
    "    max_frames = max(f.shape[1] for f in features)\n",
    "    padded_features = []\n",
    "    \n",
    "    for f in features:\n",
    "        if f.shape[1] < max_frames:\n",
    "            # Pad frames dimension\n",
    "            pad_amount = max_frames - f.shape[1]\n",
    "            f = torch.nn.functional.pad(f, (0, pad_amount))\n",
    "        padded_features.append(f)\n",
    "    \n",
    "    return torch.stack(padded_features), torch.LongTensor(labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_streaming,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_streaming,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"âœ… MFCC DataLoaders ready (streaming mode)\")\n",
    "print(f\"   Features: 40 MFCCs + 40 deltas = 80 channels\")\n",
    "print(f\"   Distribution: Balanced round-robin assignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb74f1",
   "metadata": {},
   "source": [
    "## Step 7: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'warmup_epochs': 3,\n",
    "    'save_every_n_epochs': 5,\n",
    "    'validate_every_n_epochs': 1,\n",
    "}\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=TRAINING_CONFIG['learning_rate'],\n",
    "    weight_decay=TRAINING_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=TRAINING_CONFIG['num_epochs'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"âœ… Training configuration ready\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b754a5",
   "metadata": {},
   "source": [
    "## Step 8: Checkpoint Resume Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_latest_checkpoint(checkpoint_dir, model, optimizer, scheduler=None):\n",
    "    \"\"\"\n",
    "    Load the latest checkpoint if available.\n",
    "    Returns: start_epoch, best_val_acc\n",
    "    \"\"\"\n",
    "    checkpoints = glob.glob(f\"{checkpoint_dir}/checkpoint_epoch*.pth\")\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        return 0, 0.0\n",
    "    \n",
    "    # Find latest checkpoint\n",
    "    latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "    \n",
    "    print(f\"âœ… Resumed from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.2f}%\\n\")\n",
    "    \n",
    "    return start_epoch, best_val_acc\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch, best_val_acc = load_latest_checkpoint(\n",
    "    checkpoint_dir, model, optimizer, scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca00e6e",
   "metadata": {},
   "source": [
    "## Step 9: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"[MFCC] Epoch {epoch+1}/{TRAINING_CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch_idx, (features, labels) in enumerate(pbar):\n",
    "        # Move to GPU\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': total_loss / num_batches,\n",
    "            'acc': 100.0 * correct / total\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(val_loader, desc=\"[MFCC] Validation\"):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting MFCC Training\")\n",
    "print(\"Expected: Val Acc 75-80% (vs 61.5% raw waveform baseline)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, TRAINING_CONFIG['num_epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = None, None\n",
    "    if (epoch + 1) % TRAINING_CONFIG['validate_every_n_epochs'] == 0:\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = f\"{checkpoint_dir}/best_model.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, best_model_path)\n",
    "            print(f\"âœ… New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Epoch summary\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\n[MFCC] Epoch {epoch+1}/{TRAINING_CONFIG['num_epochs']} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    if val_acc is not None:\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  Time: {epoch_time:.1f}s | LR: {scheduler.get_last_lr()[0]:.2e}\\n\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % TRAINING_CONFIG['save_every_n_epochs'] == 0:\n",
    "        checkpoint_path = f\"{checkpoint_dir}/checkpoint_epoch{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_acc': val_acc if val_acc else 0.0,\n",
    "            'best_val_acc': best_val_acc,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"ðŸ’¾ Checkpoint saved: {checkpoint_path}\\n\")\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MFCC Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {training_time/3600:.2f} hours\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Improvement over raw waveform: +{best_val_acc - 61.5:.2f}%\")\n",
    "print(f\"Best model saved at: {checkpoint_dir}/best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bb4fe",
   "metadata": {},
   "source": [
    "## Step 10: Export Trained Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbe113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = f\"{checkpoint_dir}/best_model.pth\"\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best MFCC model (Val Acc: {checkpoint['val_acc']:.2f}%)\\n\")\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.randn(1, 80, 300).to(device)  # 80 channels, ~300 frames\n",
    "onnx_path = f\"{checkpoint_dir}/fast_cnn_mfcc_trained.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=['mfcc_features'],\n",
    "    output_names=['speaker_logits'],\n",
    "    dynamic_axes={\n",
    "        'mfcc_features': {0: 'batch_size', 2: 'n_frames'},\n",
    "        'speaker_logits': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "print(f\"âœ… ONNX model exported: {onnx_path}\")\n",
    "print(f\"Model size: {os.path.getsize(onnx_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Download ONNX model\n",
    "from google.colab import files\n",
    "files.download(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac64966",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ MFCC Training Complete!\n",
    "\n",
    "**Results Summary**:\n",
    "- Input: 80-channel MFCC features (40 coefficients + 40 deltas)\n",
    "- Expected Val Acc: 75-80% (vs 61.5% raw waveform)\n",
    "- Training time: ~2 hours (same as raw waveform)\n",
    "\n",
    "**Next Steps**:\n",
    "1. Compare MFCC vs raw waveform performance\n",
    "2. Analyze convergence speed and stability\n",
    "3. Update TRAINING_RESULTS_SUMMARY.md with MFCC results\n",
    "4. If Val Acc â‰¥75%, proceed to production deployment\n",
    "5. If Val Acc <75%, consider X-vectors or AMI Corpus\n",
    "\n",
    "**Key Improvements**:\n",
    "- âœ… Industry-standard acoustic features\n",
    "- âœ… Better generalization (Train-Val gap should be <10%)\n",
    "- âœ… Faster convergence (peak around epoch 15-20)\n",
    "- âœ… More stable training (lower variance)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
