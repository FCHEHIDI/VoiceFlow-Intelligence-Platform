"""
Convert model to FP16 (half precision) for speedup on compatible hardware.

FP16 provides:
- 50% size reduction
- 1.5-2x CPU speedup (if CPU supports FP16)
- 2-3x GPU speedup
- <1% accuracy loss
"""

import onnx
from onnxruntime.transformers import optimizer
from onnx import numpy_helper
import numpy as np
from pathlib import Path
import time


def convert_to_fp16(input_path: Path, output_path: Path):
    """Convert ONNX model to FP16 precision."""
    
    print("=" * 60)
    print("ðŸ”§ Converting Model to FP16")
    print("=" * 60)
    
    print(f"\n1ï¸âƒ£ Loading model:")
    print(f"   â””â”€ {input_path.name}")
    
    orig_size = input_path.stat().st_size / (1024 * 1024)
    print(f"   â””â”€ Size: {orig_size:.1f} MB")
    
    # 2. Load ONNX model
    print(f"\n2ï¸âƒ£ Converting to FP16...")
    start = time.time()
    
    model = onnx.load(str(input_path))
    
    # Convert model to FP16
    from onnxconverter_common import float16
    
    try:
        model_fp16 = float16.convert_float_to_float16(
            model,
            keep_io_types=True  # Keep inputs/outputs as FP32 for compatibility
        )
    except Exception as e:
        print(f"   âš  onnxconverter_common failed: {e}")
        print(f"   â”œâ”€ Trying manual conversion...")
        
        # Manual FP16 conversion
        model_fp16 = convert_model_to_fp16_manual(model)
    
    duration = time.time() - start
    
    # 3. Save
    print(f"   â”œâ”€ Saving...")
    onnx.save(model_fp16, str(output_path))
    
    print(f"   âœ“ Conversion complete in {duration:.1f}s")
    
    # 4. Compare sizes
    if output_path.exists():
        fp16_size = output_path.stat().st_size / (1024 * 1024)
        reduction = ((orig_size - fp16_size) / orig_size) * 100
        
        print(f"\n3ï¸âƒ£ Results:")
        print(f"   â”œâ”€ Original (FP32): {orig_size:.1f} MB")
        print(f"   â”œâ”€ FP16:            {fp16_size:.1f} MB")
        print(f"   â””â”€ Reduction:       {reduction:.1f}%")
        
        return output_path
    else:
        print(f"\nâŒ Conversion failed")
        return None


def convert_model_to_fp16_manual(model):
    """Manually convert FP32 tensors to FP16."""
    
    print(f"   â”œâ”€ Converting initializers...")
    
    # Convert all FP32 initializers to FP16
    for initializer in model.graph.initializer:
        if initializer.data_type == 1:  # FP32
            # Get FP32 array
            fp32_array = numpy_helper.to_array(initializer)
            
            # Convert to FP16
            fp16_array = fp32_array.astype(np.float16)
            
            # Update initializer
            new_initializer = numpy_helper.from_array(fp16_array, initializer.name)
            initializer.CopyFrom(new_initializer)
    
    print(f"   â”œâ”€ Converted {len(model.graph.initializer)} tensors")
    
    return model


def test_fp16_model(model_path: Path):
    """Test if FP16 model works."""
    
    print(f"\n4ï¸âƒ£ Testing FP16 model...")
    
    try:
        import onnxruntime as ort
        
        # Try to load
        session = ort.InferenceSession(
            str(model_path),
            providers=['CPUExecutionProvider']
        )
        
        # Test inference
        import numpy as np
        dummy_input = np.random.randn(1, 48000).astype(np.float32)
        
        outputs = session.run(None, {'audio': dummy_input})
        
        print(f"   âœ“ FP16 model works!")
        print(f"   â””â”€ Output shape: {outputs[0].shape}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ FP16 model failed: {e}")
        return False


def main():
    # Try to import onnxconverter_common
    try:
        from onnxconverter_common import float16
        print("âœ“ onnxconverter_common available")
    except ImportError:
        print("âš  Installing onnxconverter_common...")
        import subprocess
        subprocess.run(["python", "-m", "pip", "install", "onnxconverter-common"], check=True)
        from onnxconverter_common import float16
    
    # Use the unoptimized model (smaller, easier to convert)
    input_path = Path("../models/diarization_transformer.onnx")
    output_path = Path("../models/diarization_transformer_fp16.onnx")
    
    if not input_path.exists():
        print(f"âŒ Input model not found: {input_path}")
        print(f"   Trying optimized version...")
        input_path = Path("../models/diarization_transformer_optimized.onnx")
        
        if not input_path.exists():
            print(f"âŒ No models found")
            return
    
    # Convert
    fp16_path = convert_to_fp16(input_path, output_path)
    
    if not fp16_path:
        print(f"\nâŒ Conversion failed")
        return
    
    # Test
    if test_fp16_model(fp16_path):
        print("\n" + "=" * 60)
        print("âœ… FP16 Conversion Complete!")
        print("=" * 60)
        print(f"\nFile: {fp16_path.name}")
        print(f"\nNote: FP16 speedup depends on hardware support:")
        print(f"â”œâ”€ CPU: 1.2-1.5x faster (limited FP16 support)")
        print(f"â”œâ”€ GPU: 2-3x faster (full FP16 acceleration)")
        print(f"â””â”€ Best for: CUDA GPUs, ARM CPUs (Apple Silicon, mobile)")
    else:
        print(f"\nâŒ FP16 model doesn't work on this hardware")
        print(f"   Consider GPU inference instead")


if __name__ == "__main__":
    main()
